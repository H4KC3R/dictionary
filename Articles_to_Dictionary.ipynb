{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5509455c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\daminov\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (24.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b2f2e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\daminov\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\daminov\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\daminov\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\daminov\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\daminov\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\daminov\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Requirement already satisfied: pyenchant in c:\\users\\daminov\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.2.2)\n",
      "Requirement already satisfied: pyPDF2 in c:\\users\\daminov\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: pillow in c:\\users\\daminov\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (9.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\daminov\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\daminov\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\daminov\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\daminov\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\daminov\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\daminov\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\daminov\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\daminov\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\daminov\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install pyenchant\n",
    "!pip install pyPDF2\n",
    "!pip install pillow\n",
    "!pip install requests\n",
    "!pip install beautifulsoup4\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e63eaa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d12b2e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_text(pdf_file_path):\n",
    "    text = \"\"\n",
    "    # Открываем PDF файл\n",
    "    with open(pdf_file_path, 'rb') as pdf_file:\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "\n",
    "        # # Создаем объект для записи текста в файл\n",
    "        # with open(output_txt_file, 'w', encoding='utf-8') as txt_file:\n",
    "        # Проходим по всем страницам PDF\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            # Получаем текст со страницы\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "\n",
    "            # # Записываем текст в файл\n",
    "            # txt_file.write(text)\n",
    "\n",
    "    print(\"PDF успешно сконвертирован в TXT.\")\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "043ed9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_articles(input_dir):\n",
    "    # Создаем пустую строку для объединения текста из всех статей\n",
    "    combined_text = \"\"\n",
    "\n",
    "    # Проходим по всем файлам в директории\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            # Получаем путь к файлу\n",
    "            file_path = os.path.join(input_dir, filename)\n",
    "\n",
    "            # Преобразуем PDF в текст и добавляем его к общему тексту\n",
    "            article_text = pdf_to_text(file_path)\n",
    "            combined_text += article_text\n",
    "\n",
    "    return combined_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af141f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF успешно сконвертирован в TXT.\n",
      "Все статьи успешно сконвертированы в один TXT файл.\n"
     ]
    }
   ],
   "source": [
    "# Путь к директории с PDF файлами\n",
    "input_dir = \"articles\"\n",
    "\n",
    "# Обрабатываем все статьи\n",
    "combined_text = process_articles(input_dir)\n",
    "\n",
    "# Путь для сохранения объединенного текста\n",
    "output_txt_file = \"articles.txt\"\n",
    "\n",
    "# Сохраняем объединенный текст в файл\n",
    "with open(output_txt_file, 'w', encoding='utf-8') as txt_file:\n",
    "    txt_file.write(combined_text)\n",
    "\n",
    "print(\"Все статьи успешно сконвертированы в один TXT файл.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74745615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "import enchant\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c410278",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Daminov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Daminov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Daminov\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f828282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c123482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_word(word, pos):\n",
    "    if pos is None:\n",
    "        return word\n",
    "    else:\n",
    "        lemma = nltk.WordNetLemmatizer().lemmatize(word, pos=pos)\n",
    "        return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f6c5aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_unique_words(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged_words = nltk.pos_tag(tokens)\n",
    "    unique_words = set()\n",
    "\n",
    "    for word, tag in tagged_words:\n",
    "        pos = get_wordnet_pos(tag)\n",
    "        lemma = lemmatize_word(word.lower(), pos)\n",
    "        unique_words.add(lemma)\n",
    "\n",
    "    return unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa853ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_english_words(words):\n",
    "    d = enchant.Dict(\"en_US\")\n",
    "    english_words = set()\n",
    "    for word in words:\n",
    "        # Проверяем, является ли слово английским и находится ли оно в словаре\n",
    "        if re.match(r'^[a-zA-Z\\-]+$', word) and d.check(word):\n",
    "            english_words.add(word)\n",
    "    return english_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f432fdac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Уникальные слова:\n",
      "1069\n"
     ]
    }
   ],
   "source": [
    "with open('articles.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "unique_words = extract_unique_words(text)\n",
    "filtered_wors =  filter_english_words(unique_words)\n",
    "\n",
    "with open('words.txt', 'w') as file:\n",
    "    for word in filtered_wors:\n",
    "        file.write(word + '\\n')\n",
    "\n",
    "print(\"Уникальные слова:\")\n",
    "print(len(filtered_wors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0d6cee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_newlines_and_sort(input_file_path, output_file_path):\n",
    "    with open(input_file_path, 'r') as input_file:\n",
    "        lines = input_file.readlines()\n",
    "\n",
    "    words = [word.strip() for word in lines if word.strip()]\n",
    "\n",
    "    sorted_words = sorted(words)\n",
    "\n",
    "    with open(output_file_path, 'w') as output_file:\n",
    "        for word in sorted_words:\n",
    "            output_file.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ab40130",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = 'words.txt'\n",
    "output_file_path = 'words_thinned.txt'\n",
    "remove_extra_newlines_and_sort(input_file_path, output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "29f1234e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7016966",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_translation(word):\n",
    "    # word = \"wireless\"\n",
    "    url = f\"https://www.multitran.com/m.exe?l1=1&l2=2&s={word}\"\n",
    "    response = requests.get(url)\n",
    "    html_content = response.text\n",
    "\n",
    "    # Создание объекта Beautiful Soup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Поиск элемента <body> с определенным стилем\n",
    "    body = soup.body\n",
    "    container = body.find('div', class_='container')\n",
    "    mclass_elements = container.find_all(class_='mclass160_10')[2]\n",
    "    first_table = mclass_elements.find('table')\n",
    "    rows = first_table.find_all('tr', recursive=False)\n",
    "\n",
    "    # print(first_table)\n",
    "\n",
    "    flag = 0\n",
    "\n",
    "    all_trans = []\n",
    "\n",
    "    a = 0\n",
    "\n",
    "    for i, row in enumerate(rows):\n",
    "\n",
    "        # if i < 3:\n",
    "        #     continue\n",
    "\n",
    "        if row.text.strip() == \"English thesaurus\":\n",
    "            break\n",
    "\n",
    "        if row.has_attr('height'):\n",
    "            a = 2\n",
    "\n",
    "        # Находим все элементы <td> в текущей строке\n",
    "        cells = row.find_all('td')\n",
    "        # Если в строке есть два элемента <td>, то это наши данные subj и trans\n",
    "        # if len(cells) == 2:\n",
    "        #     print(len(cells))\n",
    "        # if not cells:\n",
    "        #     continue\n",
    "\n",
    "        # l = len(cells)\n",
    "\n",
    "        if a > 0:\n",
    "            flag = 1\n",
    "            a -= 1\n",
    "            # print(\"aaaaaaaa\")\n",
    "        else:\n",
    "            if len(cells) == 2 and flag > 0:\n",
    "                trans = cells[1].text.strip()\n",
    "                trans = trans.split(';')\n",
    "                trans = [re.sub(r'\\s*\\([^)]*\\)', '', el) for el in trans]\n",
    "                if len(trans) > 4:\n",
    "                    trans = trans[:4]\n",
    "                if len(all_trans) > 10:\n",
    "                    trans = [trans[0]]\n",
    "                # print(\"subj:\", subj)\n",
    "                # print(\"trans:\", trans)\n",
    "                all_trans.append(trans)\n",
    "                # print(\"---------------------\")\n",
    "                flag -= 1\n",
    "\n",
    "\n",
    "    all_trans = [el for sub in all_trans for el in sub]\n",
    "\n",
    "    if 'stresses' in all_trans:\n",
    "        all_trans.remove('stresses')\n",
    "    return all_trans\n",
    "    # print(all_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7bc8ded3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|█████████████████████████████████████████████████████████████▍                 | 832/1069 [04:07<01:10,  3.37it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [30]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Перебираем слова и вызываем для каждого get_translation\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tqdm(words):\n\u001b[1;32m----> 9\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mget_translation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     all_res\u001b[38;5;241m.\u001b[39mappend([word, result])\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# print(result)\u001b[39;00m\n",
      "Input \u001b[1;32mIn [29]\u001b[0m, in \u001b[0;36mget_translation\u001b[1;34m(word)\u001b[0m\n\u001b[0;32m     13\u001b[0m mclass_elements \u001b[38;5;241m=\u001b[39m container\u001b[38;5;241m.\u001b[39mfind_all(class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmclass160_10\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     14\u001b[0m first_table \u001b[38;5;241m=\u001b[39m mclass_elements\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[43mfirst_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m'\u001b[39m, recursive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# print(first_table)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m flag \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "# Открываем файл для чтения\n",
    "all_res = []\n",
    "with open(\"wordsr_thinned.txt\", \"r\") as file:\n",
    "    # Читаем все содержимое файла\n",
    "    words = file.read().split()\n",
    "\n",
    "    # Перебираем слова и вызываем для каждого get_translation\n",
    "    for word in tqdm(words):\n",
    "        result = get_translation(word)\n",
    "        all_res.append([word, result])\n",
    "        # print(result)\n",
    "\n",
    "    with open('dictionary.txt', 'w') as file:\n",
    "        for res in all_res:\n",
    "            file.write(res[0] + \": \" + ', '.join(res[1]) + '.\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
