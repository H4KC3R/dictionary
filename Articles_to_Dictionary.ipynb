{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5509455c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2f2e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install pyenchant\n",
    "!pip install pyPDF2\n",
    "!pip install pillow\n",
    "!pip install requests\n",
    "!pip install beautifulsoup4\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63eaa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12b2e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_text(pdf_file_path, delete_dash_flag):\n",
    "    text = \"\"\n",
    "    result_text = \"\"\n",
    "    # Открываем PDF файл\n",
    "    with open(pdf_file_path, 'rb') as pdf_file:\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "\n",
    "        # # Создаем объект для записи текста в файл\n",
    "        # with open(output_txt_file, 'w', encoding='utf-8') as txt_file:\n",
    "        # Проходим по всем страницам PDF\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            # Получаем текст со страницы\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "\n",
    "            # # Записываем текст в файл\n",
    "            # txt_file.write(text)\n",
    "\n",
    "    if(delete_dash_flag == True):\n",
    "        result_text = text.replace(\"-\",\"\")\n",
    "    else:\n",
    "        result_text = text\n",
    "        \n",
    "    print(\"PDF успешно сконвертирован в TXT.\")\n",
    "\n",
    "    return result_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043ed9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_articles(input_dir, delete_dash_flag):\n",
    "    # Создаем пустую строку для объединения текста из всех статей\n",
    "    combined_text = \"\"\n",
    "\n",
    "    # Проходим по всем файлам в директории\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            # Получаем путь к файлу\n",
    "            file_path = os.path.join(input_dir, filename)\n",
    "\n",
    "            # Преобразуем PDF в текст и добавляем его к общему тексту\n",
    "            article_text = pdf_to_text(file_path, delete_dash_flag)\n",
    "            combined_text += article_text\n",
    "\n",
    "    return combined_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af141f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Путь к директории с PDF файлами\n",
    "input_dir = \"articles\"\n",
    "\n",
    "# Обрабатываем все статьи\n",
    "combined_text = process_articles(input_dir, True)\n",
    "\n",
    "# Путь для сохранения объединенного текста\n",
    "output_txt_file = \"articles.txt\"\n",
    "\n",
    "# Сохраняем объединенный текст в файл\n",
    "with open(output_txt_file, 'w', encoding='utf-8') as txt_file:\n",
    "    txt_file.write(combined_text)\n",
    "\n",
    "print(\"Все статьи успешно сконвертированы в один TXT файл.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74745615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "import enchant\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c410278",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f828282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c123482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_word(word, pos):\n",
    "    if pos is None:\n",
    "        return word\n",
    "    else:\n",
    "        lemma = nltk.WordNetLemmatizer().lemmatize(word, pos=pos)\n",
    "        return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6c5aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_unique_words(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged_words = nltk.pos_tag(tokens)\n",
    "    unique_words = set()\n",
    "\n",
    "    for word, tag in tagged_words:\n",
    "        pos = get_wordnet_pos(tag)\n",
    "        lemma = lemmatize_word(word.lower(), pos)\n",
    "        unique_words.add(lemma)\n",
    "\n",
    "    return unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa853ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_english_words(words):\n",
    "    d = enchant.Dict(\"en_US\")\n",
    "    english_words = set()\n",
    "    for word in words:\n",
    "        # Проверяем, является ли слово английским и находится ли оно в словаре\n",
    "        if re.match(r'^[a-zA-Z\\-]+$', word) and d.check(word):\n",
    "            english_words.add(word)\n",
    "    return english_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f432fdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('articles.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "unique_words = extract_unique_words(text)\n",
    "filtered_wors =  filter_english_words(unique_words)\n",
    "\n",
    "with open('words.txt', 'w') as file:\n",
    "    for word in filtered_wors:\n",
    "        file.write(word + '\\n')\n",
    "\n",
    "print(\"Уникальные слова:\")\n",
    "print(len(filtered_wors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d6cee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_newlines_and_sort(input_file_path, output_file_path):\n",
    "    with open(input_file_path, 'r') as input_file:\n",
    "        lines = input_file.readlines()\n",
    "\n",
    "    words = [word.strip() for word in lines if word.strip()]\n",
    "\n",
    "    sorted_words = sorted(words)\n",
    "\n",
    "    with open(output_file_path, 'w') as output_file:\n",
    "        for word in sorted_words:\n",
    "            output_file.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab40130",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = 'words.txt'\n",
    "output_file_path = 'words_thinned.txt'\n",
    "remove_extra_newlines_and_sort(input_file_path, output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f1234e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7016966",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_translation(word):\n",
    "    # word = \"wireless\"\n",
    "    url = f\"https://www.multitran.com/m.exe?l1=1&l2=2&s={word}\"\n",
    "    response = requests.get(url)\n",
    "    html_content = response.text\n",
    "\n",
    "    # Создание объекта Beautiful Soup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Поиск элемента <body> с определенным стилем\n",
    "    body = soup.body\n",
    "    container = body.find('div', class_='container')\n",
    "    mclass_elements = container.find_all(class_='mclass160_10')[2]\n",
    "    first_table = mclass_elements.find('table')\n",
    "    rows = first_table.find_all('tr', recursive=False)\n",
    "\n",
    "    # print(first_table)\n",
    "\n",
    "    flag = 0\n",
    "\n",
    "    all_trans = []\n",
    "\n",
    "    a = 0\n",
    "\n",
    "    for i, row in enumerate(rows):\n",
    "\n",
    "        # if i < 3:\n",
    "        #     continue\n",
    "\n",
    "        if row.text.strip() == \"English thesaurus\":\n",
    "            break\n",
    "\n",
    "        if row.has_attr('height'):\n",
    "            a = 2\n",
    "\n",
    "        # Находим все элементы <td> в текущей строке\n",
    "        cells = row.find_all('td')\n",
    "        # Если в строке есть два элемента <td>, то это наши данные subj и trans\n",
    "        # if len(cells) == 2:\n",
    "        #     print(len(cells))\n",
    "        # if not cells:\n",
    "        #     continue\n",
    "\n",
    "        # l = len(cells)\n",
    "\n",
    "        if a > 0:\n",
    "            flag = 1\n",
    "            a -= 1\n",
    "            # print(\"aaaaaaaa\")\n",
    "        else:\n",
    "            if len(cells) == 2 and flag > 0:\n",
    "                trans = cells[1].text.strip()\n",
    "                trans = trans.split(';')\n",
    "                trans = [re.sub(r'\\s*\\([^)]*\\)', '', el) for el in trans]\n",
    "                if len(trans) > 4:\n",
    "                    trans = trans[:4]\n",
    "                if len(all_trans) > 10:\n",
    "                    trans = [trans[0]]\n",
    "                # print(\"subj:\", subj)\n",
    "                # print(\"trans:\", trans)\n",
    "                all_trans.append(trans)\n",
    "                # print(\"---------------------\")\n",
    "                flag -= 1\n",
    "\n",
    "\n",
    "    all_trans = [el for sub in all_trans for el in sub]\n",
    "\n",
    "    if 'stresses' in all_trans:\n",
    "        all_trans.remove('stresses')\n",
    "    return all_trans\n",
    "    # print(all_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80df4df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Открываем файл для чтения\n",
    "all_res = []\n",
    "with open(\"words_thinned.txt\", \"r\") as file:\n",
    "    # Читаем все содержимое файла\n",
    "    words = file.read().split()\n",
    "    \n",
    "    # Перебираем слова и вызываем для каждого get_translation\n",
    "    for word in tqdm(words):\n",
    "        result = get_translation(word)\n",
    "        all_res.append([word, result])\n",
    "        # print(result)\n",
    "\n",
    "with open('dictionary_radmir.txt', 'w') as file:\n",
    "    for res in all_res:\n",
    "        file.write(res[0] + \": \" + ', '.join(res[1]) + '.\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156a6d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dictionary.txt', 'w') as file:\n",
    "    for res in all_res:\n",
    "        #print(res[1])\n",
    "        file.write(res[0] + \": \")\n",
    "        for i in range(0, len(res[1])):\n",
    "            try:\n",
    "                if(i == len(res[1])-1):\n",
    "                    file.write(res[1][i] + \".\\n\")\n",
    "                else:\n",
    "                    file.write(res[1][i] +\", \")\n",
    "            except UnicodeEncodeError:\n",
    "                continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
